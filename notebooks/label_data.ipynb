{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51884da7-5d9a-4982-a816-6723bf1046a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      ['ተገጣጣሚዎቹን', 'የእንጨት', 'ብሎኮች', 'ከመነሻዬ', 'ሲሸምቱ',...\n",
      "1      ['ስለ', 'ህዋ', 'ሳይንስ', 'የሚያስተምሩ', 'ትምህርታዊ', 'መጫዎ...\n",
      "2      ['የካሪስማ', 'ደስታ', 'በመነሻዬ', 'መነሻዬ', 'ለልጆች', 'የሚሆ...\n",
      "3      ['አና', 'ደስታዋ', 'እጥፍ', 'ድርብ', 'ሆኗል', 'ያለምንም', '...\n",
      "4      ['ውድ', 'ደንበኞቻችን', 'ከዚህ', 'በታች', 'ያስቀመጥንልዎ', 'ለ...\n",
      "                             ...                        \n",
      "723    ['ስለ', 'ልጆዎ', 'ግድ', 'ስለሚለን', 'የምናገለግሎ', 'በእውቀት...\n",
      "724    ['መነሻዬ', 'ትምህርታዊ', 'መጫወቻዎችን', 'የሚያገኙበት', 'ምርጥ'...\n",
      "725                             ['መነሻዬ', 'በክብር', 'ተከፈተ']\n",
      "726    ['መነሻዬ', 'ዋጋ', 'ብር', 'መነሻዬ', 'የልጆች', 'መማርያ', '...\n",
      "727    ['መነሻዬ', 'ዋጋ', 'ብር', 'ከድንጋይ', 'በተሰራ', 'ወረቀት', ...\n",
      "Name: processed_message, Length: 728, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/scraped_data.csv()')\n",
    "framed_data = pd.DataFrame(data)\n",
    "print(framed_data['processed_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f571c5ad-1453-40fa-9bf4-f56fc425dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = ['B-LANGUAGE', 'B-LANGUAGE', 'B-PERSON', 'I-PERSON', 'B-ORG', 'B-EVENT', 'B-LOC', 'B-LOC', 'B-FAMILY', 'B-ORG', 'B-DATE', 'I-DATE', 'B-ORG', 'B-GENDER', 'B-GENDER', 'B-PERSON', 'B-ORG', 'B-ORG', 'B-QUANTITY', 'B-MONEY', 'B-MONEY', 'B-ORG', 'B-ORG', 'B-PERCENT', 'B-DATE', 'B-DATE', 'B-EVENT', 'B-ORG', 'B-ORG', 'B-PERSON', 'B-QUANTITY', 'B-ORG', 'B-QUANTITY', 'B-DATE', 'B-LOC', 'B-EVENT', 'B-LOC', 'B-LOC', 'B-EVENT', 'B-LOC', 'B-DATE', 'B-ORG', 'B-ORG', 'B-QUANTITY', 'B-MONEY', 'B-ORG', 'B-DATE', 'B-DATE', 'B-DATE', 'B-LOC', 'B-QUANTITY', 'B-QUANTITY']\n",
    "\n",
    "entity = ['አማርኛ', 'እንግሊዝኛ', 'ዮና', 'ስም', 'የውሃ አሳይ', 'የዓለም ጨዋታ', 'ግርግር', 'ኢትዮጵያ', 'ቤተሰብ', 'ኦርቶዶካስ', 'ዕለት', 'እንቅስቃሴ', 'ወንድ', 'ሴት', 'ደቂቀ ንጉስ', 'ማዕከል', 'ወንዌል', 'በሉቀ', '20', 'ኪሎ', 'ገንዘብ', '500', 'ማቅረብ', 'እንቁላል', 'ወታደር', 'ወታደርነት', 'ዋጋ', '50', '%', 'ሰንበት', 'ተማሪ', 'እንቁላል', 'የቅዱስ ወይዘር', 'አምላክ', 'ዓለም', 'ነገ', 'ታሪክ', 'ምን', 'ዓለም', 'ወቅታዊ', 'ጉብኝት', 'ተወካይ', 'አሰባሳት', 'ደህንነት', 'አሳያ', 'እረፍታ', 'ወገን', 'ግንባር', 'አቅም', 'ሂወት']\n",
    "# Define the file path\n",
    "filepath = '../data/labled.csv'\n",
    "# Open the file in write mode with UTF-8 encoding\n",
    "with open(filepath, 'w', encoding='utf-8') as labeled_data:\n",
    "    for label, entity in zip(labels, data):\n",
    "        labeled_data.write(f\"{entity} ':' {label}\\n\")  # Write each entity and label pair to the file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ec6602-1146-4f78-95da-8e41f0b6d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20864\\2115981822.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  tokens = eval(row[0])  # Convert string list back to Python list\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenized data from the CSV file\n",
    "filepath = '../data/scraped_data.csv()'\n",
    "tokenized_data_frame = pd.read_csv(filepath)\n",
    "\n",
    "# Define the labeling function using if...else\n",
    "def label_token(token):\n",
    "    if token == \"ተገጣጣሚዎቹን\":\n",
    "        return \"O\"\n",
    "    elif token == \"ትምህርታዊ\":\n",
    "        return \"O\"\n",
    "    elif token == \"መጫዎቻዎች\":\n",
    "        return \"B-PRODUCT\"\n",
    "    elif token == \"መሳሪያ\":\n",
    "        return \"I-PRODUCT\"\n",
    "    elif token == \"ውድ\":\n",
    "        return \"B-PERSON\"\n",
    "    elif token == \"ልጆችዎ\":\n",
    "        return \"B-FAMILY\"\n",
    "    elif token == \"አቀፍ ዝናብ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"ይማራሉ\":\n",
    "        return \"B-EVENT\"\n",
    "    elif token == \"አማራጮች\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"ተለያዩ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"የሚያገኙበት\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"እውነተኛ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"ይደውሉ\":\n",
    "        return \"B-EVENT\"\n",
    "    elif token == \"አድራሻ\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"ሆሊ\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"ሲቲ\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"ደስታ\":\n",
    "        return \"B-EVENT\"\n",
    "    elif token == \"እንደ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"ርዕሰ ጉዞ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"ይጎብኙ\":\n",
    "        return \"B-EVENT\"\n",
    "    else:\n",
    "        return \"O\"  # Default label for out-of-scope tokens\n",
    "\n",
    "# Create a list to hold labeled data\n",
    "labeled_data = []\n",
    "\n",
    "# Apply the labeling function to each row and expand the results\n",
    "for index, row in tokenized_data_frame.iterrows():\n",
    "    # Assuming the first column contains the tokenized list of words\n",
    "    tokens = eval(row[0])  # Convert string list back to Python list\n",
    "    for token in tokens:\n",
    "        label = label_token(token)\n",
    "        labeled_data.append((token, label))  # Append token and its label as a tuple\n",
    "\n",
    "# Convert to DataFrame for easier saving or further processing\n",
    "labeled_df = pd.DataFrame(labeled_data, columns=['Labeled Data', 'ner_tags'])\n",
    "\n",
    "# Save the labeled data to a CSV file\n",
    "labeled_df.to_csv('../data/labeled_data.csv', index=False)\n",
    "\n",
    "print(\"Labeled data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92fc6d88-98c7-4ab4-b4d8-bed999205677",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, load_metric  \u001b[38;5;66;03m# Ensure load_metric is imported\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric  # Ensure load_metric is imported\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Your Labeled Dataset\n",
    "labeled_data = pd.read_csv('../data/labeled_data.csv', encoding='utf-8')\n",
    "\n",
    "# Step 2: Create a sentence ID to group tokens by sentence\n",
    "labeled_data['sentence_id'] = labeled_data.index // 10  # Adjust based on your sentence length\n",
    "\n",
    "# Step 3: Group tokens and labels by sentence\n",
    "grouped_data = labeled_data.groupby('sentence_id').agg({'Labeled Data': list, 'ner_tags': list}).reset_index()\n",
    "\n",
    "# Verify the grouped data structure\n",
    "print(grouped_data.head())\n",
    "\n",
    "# Step 4: Convert the DataFrame into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(grouped_data)\n",
    "\n",
    "# Check the dataset columns\n",
    "print(dataset.column_names)\n",
    "\n",
    "# Step 5: Load Pre-trained Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Step 6: Define Label Mapping\n",
    "label_names = list(set(labeled_data['ner_tags']))  # Extract unique labels from your dataset\n",
    "label_map = {label: idx for idx, label in enumerate(label_names)}  # Create a mapping of label to index\n",
    "\n",
    "# Step 7: Tokenization and Label Alignment Function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Accessing the columns directly to ensure they are correctly named\n",
    "    tokenized_inputs = tokenizer(examples['Labeled Data'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):  # Use ner_tags here\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])  # Convert label to its corresponding integer\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to other tokens of the same word\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels  # Store labels in 'labels'\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Step 8: Apply the Tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Step 9: Load the Pre-trained Model for Token Classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_names))  # Use the dynamic label count\n",
    "\n",
    "# Step 10: Define Evaluation Metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "# Step 11: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    ")\n",
    "\n",
    "# Step 12: Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Assuming a single dataset for training\n",
    "    eval_dataset=tokenized_datasets,  # Using the same for evaluation; modify if needed\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 13: Evaluate the Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Step 14: Inference Example\n",
    "sentence = \"አቶ ሳሙኤል አያሌው ኢትዮጵያ የመንግሥት ተቋም ነው።\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9ad679-708a-4723-98f1-9279aa23dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id                                       Labeled Data  \\\n",
      "0            0  [ተገጣጣሚዎቹን, የእንጨት, ብሎኮች, ከመነሻዬ, ሲሸምቱ, ልጆችዎ, ያለጥ...   \n",
      "1            1  [ብሎኮችን, በተለያዩ, አማራጮች, ለማግኘት, ይጫኑ, አድራሻ, ጉርድ, ሾ...   \n",
      "2            2  [ሴንተር, ፎቅ, ይደውሉ, ወይም, መነሻዬ, የእንጨትብሎኮች, ትምህርታዊመ...   \n",
      "3            3  [የሚያስተምሩ, ትምህርታዊ, መጫዎቻዎችን, በተለያዩ, አማራጮች, ከመነሻዬ...   \n",
      "4            4  [የሚያስተምር, ትምህርታዊ, መጫወቻዎችን, ለማግኘት, ይጫኑ, አድራሻ, ጉ...   \n",
      "\n",
      "                                         ner_tags  \n",
      "0     [O, O, O, O, O, B-FAMILY, O, O, B-EVENT, O]  \n",
      "1  [O, O, B-ORG, O, O, B-LOC, O, O, B-LOC, B-LOC]  \n",
      "2            [O, O, B-EVENT, O, O, O, O, O, O, O]  \n",
      "3              [O, O, O, O, B-ORG, O, O, O, O, O]  \n",
      "4      [O, O, O, O, O, B-LOC, O, O, B-LOC, B-LOC]  \n",
      "['sentence_id', 'Labeled Data', 'ner_tags']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd00801be634063b2f7d100ac717cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/768 04:54 < 13:21, 0.70 it/s, Epoch 0.81/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 12 at dim 1 (got 3)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 98\u001b[0m\n\u001b[0;32m     88\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     89\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     90\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Step 13: Evaluate the Model\u001b[39;00m\n\u001b[0;32m    101\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2345\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m--> 561\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3544\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3541\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3542\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:791\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from evaluate import load  # Correctly import load from evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Your Labeled Dataset\n",
    "labeled_data = pd.read_csv('../data/labeled_data.csv', encoding='utf-8')\n",
    "\n",
    "# Step 2: Create a sentence ID to group tokens by sentence\n",
    "labeled_data['sentence_id'] = labeled_data.index // 10  # Adjust based on your sentence length\n",
    "\n",
    "# Step 3: Group tokens and labels by sentence\n",
    "grouped_data = labeled_data.groupby('sentence_id').agg({'Labeled Data': list, 'ner_tags': list}).reset_index()\n",
    "\n",
    "# Verify the grouped data structure\n",
    "print(grouped_data.head())\n",
    "\n",
    "# Step 4: Convert the DataFrame into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(grouped_data)\n",
    "\n",
    "# Check the dataset columns\n",
    "print(dataset.column_names)\n",
    "\n",
    "# Step 5: Load Pre-trained Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Step 6: Define Label Mapping\n",
    "label_names = list(set(labeled_data['ner_tags']))  # Extract unique labels from your dataset\n",
    "label_map = {label: idx for idx, label in enumerate(label_names)}  # Create a mapping of label to index\n",
    "\n",
    "# Step 7: Tokenization and Label Alignment Function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Accessing the columns directly to ensure they are correctly named\n",
    "    tokenized_inputs = tokenizer(examples['Labeled Data'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):  # Use ner_tags here\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])  # Convert label to its corresponding integer\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to other tokens of the same word\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels  # Store labels in 'labels'\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Step 8: Apply the Tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Step 9: Load the Pre-trained Model for Token Classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_names))  # Use the dynamic label count\n",
    "\n",
    "# Step 10: Define Evaluation Metric\n",
    "metric = load(\"seqeval\")  # Load the 'seqeval' metric\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "# Step 11: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    ")\n",
    "\n",
    "# Step 12: Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Assuming a single dataset for training\n",
    "    eval_dataset=tokenized_datasets,  # Using the same for evaluation; modify if needed\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 13: Evaluate the Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Step 14: Inference Example\n",
    "sentence = \"አቶ ሳሙኤል አያሌው ኢትዮጵያ የመንግሥት ተቋም ነው።\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b44c0e-6de6-44f4-8292-92d9c07819dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
