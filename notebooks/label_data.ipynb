{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51884da7-5d9a-4982-a816-6723bf1046a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      ['·â∞·åà·å£·å£·àö·ãé·âπ·äï', '·ã®·ä•·äï·å®·âµ', '·â•·àé·äÆ·âΩ', '·ä®·àò·äê·àª·ã¨', '·à≤·à∏·àù·â±',...\n",
      "1      ['·àµ·àà', '·àÖ·ãã', '·à≥·ã≠·äï·àµ', '·ã®·àö·ã´·àµ·â∞·àù·à©', '·âµ·àù·àÖ·à≠·â≥·ãä', '·àò·å´·ãé...\n",
      "2      ['·ã®·ä´·à™·àµ·àõ', '·ã∞·àµ·â≥', '·â†·àò·äê·àª·ã¨', '·àò·äê·àª·ã¨', '·àà·àç·åÜ·âΩ', '·ã®·àö·àÜ...\n",
      "3      ['·ä†·äì', '·ã∞·àµ·â≥·ãã', '·ä•·å•·çç', '·ãµ·à≠·â•', '·àÜ·äó·àç', '·ã´·àà·àù·äï·àù', '...\n",
      "4      ['·ãç·ãµ', '·ã∞·äï·â†·äû·âª·âΩ·äï', '·ä®·ãö·àÖ', '·â†·â≥·âΩ', '·ã´·àµ·âÄ·àò·å•·äï·àç·ãé', '·àà...\n",
      "                             ...                        \n",
      "723    ['·àµ·àà', '·àç·åÜ·ãé', '·åç·ãµ', '·àµ·àà·àö·àà·äï', '·ã®·àù·äì·åà·àà·åç·àé', '·â†·ä•·ãç·âÄ·âµ...\n",
      "724    ['·àò·äê·àª·ã¨', '·âµ·àù·àÖ·à≠·â≥·ãä', '·àò·å´·ãà·âª·ãé·âΩ·äï', '·ã®·àö·ã´·åà·äô·â†·âµ', '·àù·à≠·å•'...\n",
      "725                             ['·àò·äê·àª·ã¨', '·â†·ä≠·â•·à≠', '·â∞·ä®·çà·â∞']\n",
      "726    ['·àò·äê·àª·ã¨', '·ãã·åã', '·â•·à≠', '·àò·äê·àª·ã¨', '·ã®·àç·åÜ·âΩ', '·àò·àõ·à≠·ã´', '...\n",
      "727    ['·àò·äê·àª·ã¨', '·ãã·åã', '·â•·à≠', '·ä®·ãµ·äï·åã·ã≠', '·â†·â∞·à∞·à´', '·ãà·à®·âÄ·âµ', ...\n",
      "Name: processed_message, Length: 728, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/scraped_data.csv()')\n",
    "framed_data = pd.DataFrame(data)\n",
    "print(framed_data['processed_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f571c5ad-1453-40fa-9bf4-f56fc425dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = ['B-LANGUAGE', 'B-LANGUAGE', 'B-PERSON', 'I-PERSON', 'B-ORG', 'B-EVENT', 'B-LOC', 'B-LOC', 'B-FAMILY', 'B-ORG', 'B-DATE', 'I-DATE', 'B-ORG', 'B-GENDER', 'B-GENDER', 'B-PERSON', 'B-ORG', 'B-ORG', 'B-QUANTITY', 'B-MONEY', 'B-MONEY', 'B-ORG', 'B-ORG', 'B-PERCENT', 'B-DATE', 'B-DATE', 'B-EVENT', 'B-ORG', 'B-ORG', 'B-PERSON', 'B-QUANTITY', 'B-ORG', 'B-QUANTITY', 'B-DATE', 'B-LOC', 'B-EVENT', 'B-LOC', 'B-LOC', 'B-EVENT', 'B-LOC', 'B-DATE', 'B-ORG', 'B-ORG', 'B-QUANTITY', 'B-MONEY', 'B-ORG', 'B-DATE', 'B-DATE', 'B-DATE', 'B-LOC', 'B-QUANTITY', 'B-QUANTITY']\n",
    "\n",
    "entity = ['·ä†·àõ·à≠·äõ', '·ä•·äï·åç·àä·ãù·äõ', '·ãÆ·äì', '·àµ·àù', '·ã®·ãç·àÉ ·ä†·à≥·ã≠', '·ã®·ãì·àà·àù ·å®·ãã·â≥', '·åç·à≠·åç·à≠', '·ä¢·âµ·ãÆ·åµ·ã´', '·â§·â∞·à∞·â•', '·ä¶·à≠·â∂·ã∂·ä´·àµ', '·ãï·àà·âµ', '·ä•·äï·âÖ·àµ·âÉ·à¥', '·ãà·äï·ãµ', '·à¥·âµ', '·ã∞·âÇ·âÄ ·äï·åâ·àµ', '·àõ·ãï·ä®·àç', '·ãà·äï·ãå·àç', '·â†·àâ·âÄ', '20', '·ä™·àé', '·åà·äï·ãò·â•', '500', '·àõ·âÖ·à®·â•', '·ä•·äï·âÅ·àã·àç', '·ãà·â≥·ã∞·à≠', '·ãà·â≥·ã∞·à≠·äê·âµ', '·ãã·åã', '50', '%', '·à∞·äï·â†·âµ', '·â∞·àõ·à™', '·ä•·äï·âÅ·àã·àç', '·ã®·âÖ·ã±·àµ ·ãà·ã≠·ãò·à≠', '·ä†·àù·àã·ä≠', '·ãì·àà·àù', '·äê·åà', '·â≥·à™·ä≠', '·àù·äï', '·ãì·àà·àù', '·ãà·âÖ·â≥·ãä', '·åâ·â•·äù·âµ', '·â∞·ãà·ä´·ã≠', '·ä†·à∞·â£·à≥·âµ', '·ã∞·àÖ·äï·äê·âµ', '·ä†·à≥·ã´', '·ä•·à®·çç·â≥', '·ãà·åà·äï', '·åç·äï·â£·à≠', '·ä†·âÖ·àù', '·àÇ·ãà·âµ']\n",
    "# Define the file path\n",
    "filepath = '../data/labled.csv'\n",
    "# Open the file in write mode with UTF-8 encoding\n",
    "with open(filepath, 'w', encoding='utf-8') as labeled_data:\n",
    "    for label, entity in zip(labels, data):\n",
    "        labeled_data.write(f\"{entity} ':' {label}\\n\")  # Write each entity and label pair to the file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ec6602-1146-4f78-95da-8e41f0b6d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20864\\2115981822.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  tokens = eval(row[0])  # Convert string list back to Python list\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenized data from the CSV file\n",
    "filepath = '../data/scraped_data.csv()'\n",
    "tokenized_data_frame = pd.read_csv(filepath)\n",
    "\n",
    "# Define the labeling function using if...else\n",
    "def label_token(token):\n",
    "    if token == \"·â∞·åà·å£·å£·àö·ãé·âπ·äï\":\n",
    "        return \"O\"\n",
    "    elif token == \"·âµ·àù·àÖ·à≠·â≥·ãä\":\n",
    "        return \"O\"\n",
    "    elif token == \"·àò·å´·ãé·âª·ãé·âΩ\":\n",
    "        return \"B-PRODUCT\"\n",
    "    elif token == \"·àò·à≥·à™·ã´\":\n",
    "        return \"I-PRODUCT\"\n",
    "    elif token == \"·ãç·ãµ\":\n",
    "        return \"B-PERSON\"\n",
    "    elif token == \"·àç·åÜ·âΩ·ãé\":\n",
    "        return \"B-FAMILY\"\n",
    "    elif token == \"·ä†·âÄ·çç ·ãù·äì·â•\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"·ã≠·àõ·à´·àâ\":\n",
    "        return \"B-EVENT\"\n",
    "    elif token == \"·ä†·àõ·à´·åÆ·âΩ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"·â∞·àà·ã´·ã©\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"·ã®·àö·ã´·åà·äô·â†·âµ\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"·ä•·ãç·äê·â∞·äõ\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"·ã≠·ã∞·ãç·àâ\":\n",
    "        return \"B-EVENT\"\n",
    "    elif token == \"·ä†·ãµ·à´·àª\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"·àÜ·àä\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"·à≤·â≤\":\n",
    "        return \"B-LOC\"\n",
    "    elif token == \"·ã∞·àµ·â≥\":\n",
    "        return \"B-EVENT\"\n",
    "    elif token == \"·ä•·äï·ã∞\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"·à≠·ãï·à∞ ·åâ·ãû\":\n",
    "        return \"B-ORG\"\n",
    "    elif token == \"·ã≠·åé·â•·äô\":\n",
    "        return \"B-EVENT\"\n",
    "    else:\n",
    "        return \"O\"  # Default label for out-of-scope tokens\n",
    "\n",
    "# Create a list to hold labeled data\n",
    "labeled_data = []\n",
    "\n",
    "# Apply the labeling function to each row and expand the results\n",
    "for index, row in tokenized_data_frame.iterrows():\n",
    "    # Assuming the first column contains the tokenized list of words\n",
    "    tokens = eval(row[0])  # Convert string list back to Python list\n",
    "    for token in tokens:\n",
    "        label = label_token(token)\n",
    "        labeled_data.append((token, label))  # Append token and its label as a tuple\n",
    "\n",
    "# Convert to DataFrame for easier saving or further processing\n",
    "labeled_df = pd.DataFrame(labeled_data, columns=['Labeled Data', 'ner_tags'])\n",
    "\n",
    "# Save the labeled data to a CSV file\n",
    "labeled_df.to_csv('../data/labeled_data.csv', index=False)\n",
    "\n",
    "print(\"Labeled data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92fc6d88-98c7-4ab4-b4d8-bed999205677",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, load_metric  \u001b[38;5;66;03m# Ensure load_metric is imported\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric  # Ensure load_metric is imported\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Your Labeled Dataset\n",
    "labeled_data = pd.read_csv('../data/labeled_data.csv', encoding='utf-8')\n",
    "\n",
    "# Step 2: Create a sentence ID to group tokens by sentence\n",
    "labeled_data['sentence_id'] = labeled_data.index // 10  # Adjust based on your sentence length\n",
    "\n",
    "# Step 3: Group tokens and labels by sentence\n",
    "grouped_data = labeled_data.groupby('sentence_id').agg({'Labeled Data': list, 'ner_tags': list}).reset_index()\n",
    "\n",
    "# Verify the grouped data structure\n",
    "print(grouped_data.head())\n",
    "\n",
    "# Step 4: Convert the DataFrame into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(grouped_data)\n",
    "\n",
    "# Check the dataset columns\n",
    "print(dataset.column_names)\n",
    "\n",
    "# Step 5: Load Pre-trained Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Step 6: Define Label Mapping\n",
    "label_names = list(set(labeled_data['ner_tags']))  # Extract unique labels from your dataset\n",
    "label_map = {label: idx for idx, label in enumerate(label_names)}  # Create a mapping of label to index\n",
    "\n",
    "# Step 7: Tokenization and Label Alignment Function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Accessing the columns directly to ensure they are correctly named\n",
    "    tokenized_inputs = tokenizer(examples['Labeled Data'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):  # Use ner_tags here\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])  # Convert label to its corresponding integer\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to other tokens of the same word\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels  # Store labels in 'labels'\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Step 8: Apply the Tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Step 9: Load the Pre-trained Model for Token Classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_names))  # Use the dynamic label count\n",
    "\n",
    "# Step 10: Define Evaluation Metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "# Step 11: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    ")\n",
    "\n",
    "# Step 12: Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Assuming a single dataset for training\n",
    "    eval_dataset=tokenized_datasets,  # Using the same for evaluation; modify if needed\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 13: Evaluate the Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Step 14: Inference Example\n",
    "sentence = \"·ä†·â∂ ·à≥·àô·ä§·àç ·ä†·ã´·àå·ãç ·ä¢·âµ·ãÆ·åµ·ã´ ·ã®·àò·äï·åç·à•·âµ ·â∞·âã·àù ·äê·ãç·ç¢\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9ad679-708a-4723-98f1-9279aa23dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id                                       Labeled Data  \\\n",
      "0            0  [·â∞·åà·å£·å£·àö·ãé·âπ·äï, ·ã®·ä•·äï·å®·âµ, ·â•·àé·äÆ·âΩ, ·ä®·àò·äê·àª·ã¨, ·à≤·à∏·àù·â±, ·àç·åÜ·âΩ·ãé, ·ã´·àà·å•...   \n",
      "1            1  [·â•·àé·äÆ·âΩ·äï, ·â†·â∞·àà·ã´·ã©, ·ä†·àõ·à´·åÆ·âΩ, ·àà·àõ·åç·äò·âµ, ·ã≠·å´·äë, ·ä†·ãµ·à´·àª, ·åâ·à≠·ãµ, ·àæ...   \n",
      "2            2  [·à¥·äï·â∞·à≠, ·çé·âÖ, ·ã≠·ã∞·ãç·àâ, ·ãà·ã≠·àù, ·àò·äê·àª·ã¨, ·ã®·ä•·äï·å®·âµ·â•·àé·äÆ·âΩ, ·âµ·àù·àÖ·à≠·â≥·ãä·àò...   \n",
      "3            3  [·ã®·àö·ã´·àµ·â∞·àù·à©, ·âµ·àù·àÖ·à≠·â≥·ãä, ·àò·å´·ãé·âª·ãé·âΩ·äï, ·â†·â∞·àà·ã´·ã©, ·ä†·àõ·à´·åÆ·âΩ, ·ä®·àò·äê·àª·ã¨...   \n",
      "4            4  [·ã®·àö·ã´·àµ·â∞·àù·à≠, ·âµ·àù·àÖ·à≠·â≥·ãä, ·àò·å´·ãà·âª·ãé·âΩ·äï, ·àà·àõ·åç·äò·âµ, ·ã≠·å´·äë, ·ä†·ãµ·à´·àª, ·åâ...   \n",
      "\n",
      "                                         ner_tags  \n",
      "0     [O, O, O, O, O, B-FAMILY, O, O, B-EVENT, O]  \n",
      "1  [O, O, B-ORG, O, O, B-LOC, O, O, B-LOC, B-LOC]  \n",
      "2            [O, O, B-EVENT, O, O, O, O, O, O, O]  \n",
      "3              [O, O, O, O, B-ORG, O, O, O, O, O]  \n",
      "4      [O, O, O, O, O, B-LOC, O, O, B-LOC, B-LOC]  \n",
      "['sentence_id', 'Labeled Data', 'ner_tags']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd00801be634063b2f7d100ac717cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/768 04:54 < 13:21, 0.70 it/s, Epoch 0.81/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 12 at dim 1 (got 3)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 98\u001b[0m\n\u001b[0;32m     88\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     89\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     90\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Step 13: Evaluate the Model\u001b[39;00m\n\u001b[0;32m    101\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2345\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m--> 561\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3544\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3541\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3542\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:791\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from evaluate import load  # Correctly import load from evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Your Labeled Dataset\n",
    "labeled_data = pd.read_csv('../data/labeled_data.csv', encoding='utf-8')\n",
    "\n",
    "# Step 2: Create a sentence ID to group tokens by sentence\n",
    "labeled_data['sentence_id'] = labeled_data.index // 10  # Adjust based on your sentence length\n",
    "\n",
    "# Step 3: Group tokens and labels by sentence\n",
    "grouped_data = labeled_data.groupby('sentence_id').agg({'Labeled Data': list, 'ner_tags': list}).reset_index()\n",
    "\n",
    "# Verify the grouped data structure\n",
    "print(grouped_data.head())\n",
    "\n",
    "# Step 4: Convert the DataFrame into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(grouped_data)\n",
    "\n",
    "# Check the dataset columns\n",
    "print(dataset.column_names)\n",
    "\n",
    "# Step 5: Load Pre-trained Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Step 6: Define Label Mapping\n",
    "label_names = list(set(labeled_data['ner_tags']))  # Extract unique labels from your dataset\n",
    "label_map = {label: idx for idx, label in enumerate(label_names)}  # Create a mapping of label to index\n",
    "\n",
    "# Step 7: Tokenization and Label Alignment Function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Accessing the columns directly to ensure they are correctly named\n",
    "    tokenized_inputs = tokenizer(examples['Labeled Data'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):  # Use ner_tags here\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])  # Convert label to its corresponding integer\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to other tokens of the same word\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels  # Store labels in 'labels'\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Step 8: Apply the Tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Step 9: Load the Pre-trained Model for Token Classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_names))  # Use the dynamic label count\n",
    "\n",
    "# Step 10: Define Evaluation Metric\n",
    "metric = load(\"seqeval\")  # Load the 'seqeval' metric\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "# Step 11: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    ")\n",
    "\n",
    "# Step 12: Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Assuming a single dataset for training\n",
    "    eval_dataset=tokenized_datasets,  # Using the same for evaluation; modify if needed\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 13: Evaluate the Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Step 14: Inference Example\n",
    "sentence = \"·ä†·â∂ ·à≥·àô·ä§·àç ·ä†·ã´·àå·ãç ·ä¢·âµ·ãÆ·åµ·ã´ ·ã®·àò·äï·åç·à•·âµ ·â∞·âã·àù ·äê·ãç·ç¢\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b44c0e-6de6-44f4-8292-92d9c07819dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
